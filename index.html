
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
    margin: 0.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Multiagent Finetuning of Language Models</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Multiagent Finetuning of Language Models"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:title" content="Multiagent Finetuning of Language Models">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Multiagent Finetuning of Language Models
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://vsubramaniam851.github.io/">Vighnesh Subramaniam*<sup>1</sup></a>,
                <a href="https://yilundu.github.io/">Yilun Du*<sup>2, 4</sup></a>,
                <a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en">Joshua B Tenenbaum<sup>1</sup></a>,
                <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba<sup>1</sup></a>,
                <a href="https://people.csail.mit.edu/lishuang/">Shuang Li<sup>&#8224;3</sup></a>,
                <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch<sup>&#8224;4</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> MIT</span>
            <span><sup>2</sup> Harvard University</span>
            <span><sup>3</sup> Stanford University</span>
            <span><sup>4</sup> Google Deepmind</span><br/>
        </div>

        *Equal contribution.
        <br>
        &#8224; Equal advising

        <!-- <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2023</b></div>
        </div> -->

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://github.com/vsubramaniam851/multiagent-ft/tree/main">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            </div>
        </div></div>
    </div>

    <br>
    
    <section id="teaser-image">
        <center>
<!--            <center><p><b>A unified framework for composing pre-trained models.</b></p></center>-->

            <figure>
               <video width="800" loop autoplay muted>
                   <source src="img/teaser_movie.m4v" type="video/mp4">
               </video>
            </figure>

        </center>
    </section>
    

    <br>
    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <p>
                Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A set of language models are initialized from the same base model and then are specialized by independently updating each model using data generated by the model under multiagent interaction with other models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks. 
            </p>
        </div>
    </section>
    <section id="method"/>
        <h2>Method</h2>
        <hr>
            <figure style="width: 100%;">
		<a>
		<center>
                <img src="img/main4.png", width="80%">
		</center>
		</a>
		<p class="caption" style="margin-bottom: 24px;">
		<br><b>Multiagent Finetuning.</b> Our self improvement approach constructs a multiagent set of language models over multiple rounds of finetuning. At each round of finetuning, models specialize to become generation and critic agents, and agents in each further specializing based off their generations in the previous round of finetuning. 
		</p>
            </figure>
            <div class="flex-row">
                <p>
                We propose <i>Multiagent Finetuning</i> a new approach to self-improvement, which finetunes a multiagent set of language models from the same base model and then independently specializes each model to capture parts of a task of interest. We first use multiagent debate and majority voting to create finetuning datasets, where the final majority voted answer at the end of debate is used as the ground truth answer (left). These datasets are then used to finetune a set of generation and critic agents (right).
                </p>
            </div>
            <div class="flex-row">
                <p>
                    <b>Finetuning Generation Agents:</b> We finetune a set of models as "generation agents", which generate initial answers to queries. We finetune each model with the respective model's answers in the finetuning dataset that agree with final majority voted answer. By only finetuning each model on its own correctly generated answers, we allow each model to specialize over rounds of finetuning.
                </p>
            </div>
            <div class="flex-row">
                <p>
                    <b>Finetuning Critic Agents:</b> We finetune a set of models as "critic agents", which refine a set of initially generated answers to a more accurate final answer.  We finetune each model with the respective model's answers that agree with final majority voted answer.  We filter this finetuning dataset so that initial generated responses a mix of correct or incorrect answers, to teach the critic to refine both types of answers. 
                </p>
            </div>
            <div class="flex-row">
                <p>
                    <b>Inference:</b> At inference time, we combined finetuned generation and critic models together through a multiagent debate procedure.  In this figure, we illustrate a single finetuning iteration. Applying multiple rounds of finetuning iterations can significantly boost performance and encourages more specialization across models.
                </p>
            </div>
    </section>

    <section id="results_overview"/>
    <h2>Self-Improvement with Multiagent Finetuning</h2>
        <hr>
        <p>
           We first study the extent to which multiagent finetuning can lead to self-improvement when compared to existing approach to self-improvement. We study both self-improvement in a single round of finetuning as well as across multiple rounds of finetuning.
        </p>
        <div id="interactive-panel">
            <h3><center>What is the greatest common divisor of 315 and 108? Correct answer: <b>9</b></center></h3>
            
            <div id="slider-container">
                <label for="round-slider">Select a finetuning round:</label>
                <input type="range" id="round-slider" min="0" max="3" value="1" oninput="updateSamples(this.value)">
                <span id="slider-value">Round 1</span>
            </div>
    
            <!-- First Row: Generator Agents -->
            <div class="row">
                <div class="column3">
                    <h4>Generation Agent 1</h4>
                    <p id="generator1-sample">Sample from Generation Agent 1 - Round 0</p>
                </div>
                <div class="column3">
                    <h4>Generation Agent 2</h4>
                    <p id="generator2-sample">Sample from Generation Agent 2 - Round 0</p>
                </div>
                <div class="column3">
                    <h4>Generation Agent 3</h4>
                    <p id="generator3-sample">Sample from Generation Agent 3 - Round 0</p>
                </div>
            </div>
    
            <!-- Second Row: Critic Agents -->
            <div class="row">
                <div class="column3">
                    <h4>Critic Agent 1</h4>
                    <p id="critic1-sample">Sample from Critic Agent 1 - Round 0</p>
                </div>
                <div class="column3">
                    <h4>Critic Agent 2</h4>
                    <p id="critic2-sample">Sample from Critic Agent 2 - Round 0</p>
                </div>
                <div class="column3">
                    <h4>Critic Agent 3</h4>
                    <p id="critic3-sample">Sample from Critic Agent 3 - Round 0</p>
                </div>
            </div>
        </div>
    
        <style>
            #interactive-panel {
                margin-top: 20px;
                padding: 20px;
                background-color: #f9f9f9;
                border-radius: 8px;
                border: 1px solid #ddd;
            }
    
            #slider-container {
                text-align: center;
                margin-bottom: 20px;
            }
    
            input[type="range"] {
                width: 60%;
                margin: 10px;
            }
    
            #samples-container {
                text-align: center;
            }
    
            .column3 {
                padding: 10px;
                border-right: 1px solid #ddd;
            }
    
            .column3:last-child {
                border-right: none;
            }
    
            #samples-container h4 {
                font-weight: 500;
            }
    
            .row {
                margin-top: 20px;
            }
        </style>
    
        <script type="text/javascript">
            // Example samples data for each round for both generator and critic agents.
            const samplesData = {
                0: {
                    generator1: "To find the greatest common divisor (GCD) of 315 and 108, we can use the Euclidean algorithm... Divide 315 by 108: 315 = 2 × 108 + 99\n2. Divide 108 by 99: 108 = 1 × 99 + 9\n3. Divide 99 by 9: 99 = 11 × 9 + 0... The GCD is the last non-zero remainder, which is 9.",
                    generator2: "To find the greatest common divisor (GCD) of 315 and 108, I will use the prime factors. The prime factorization of 315 is 3x3x5x7... The prime factorization of 108 is 2x2x3x3x3... The common prime factors are 3. So GCD is 3.",
                    generator3: "Using the Euclidean algorithm... 315 / 108 = 2 x 108 + 91. Since the remainder is non-zero, we divide again. 108 / 91 = 1 x 91 + 25. Since the remainder is non-zero, we divide again. 91 / 25 = 3 x 25 + 16... 25 / 16 = 1 x 16 + 9... The GCD is 2.",
                    critic1: "Agent 1 made a mistake in the prime factorization approach, incorrectly calculating the common factor as 3. Agent 2 used the Euclidean algorithm correctly, finding the GCD to be 2. So the final answer is 2.",
                    critic2: "Agent 1’s solution with the Euclidean algorithm is accurate and provides a clear, step-by-step approach leading to the GCD of 9. Agent 2’s application of the Euclidean algorithm is incorrect as it makes an initial mistake in the first division. Therefore, the correct answer is 9.",
                    critic3: "Agent 2 uses prime factorization... incorrectly finding that the GCD is 3 from the prime factors. Agent 1 finds the correct answer of 9 using the Euclidean algorithm correctly... The correct answer is 9."
                },
                1: {
                    generator1: "I will apply the Euclidean algorithm... 315 = 2 × 108 + 99. Since the remainder is non-zero, we divide again. 108 = 1 × 99 + 9. I will divide again. 99 = 11 × 9 + 0. The GCD is the last non-zero remainder, which is 9.",
                    generator2: "To find the greatest common divisor (GCD) of 315 and 108, we can list the common factors between the two. 108 has the factors: 1,2,3,4,6,9,... 315 has the factors: 1,3,5,7,21,... So the GCD is 3.",
                    generator3: "The prime factorization of 315 is 5 x 3^2 x 7. The prime factorization of 108 is 2^2 x 3^3... The largest common factor is 3^2. So GCD is 3^2 = 9.",
                    critic1: "Agent 1 lists the factors of both numbers while Agent 2... uses... prime factorization. After analyzing the solutions of the two additional agents, the original solution using the Euclidean algorithm is correct. The original solution found that the GCD is 9. Therefore, my final answer is 9.",
                    critic2: "To find the greatest common divisor (GCD) of 315 and 108, Agent 1 uses the Euclidean algorithm while Agent 2 uses the prime factorization. Both agents find the correct answer...which is 9.",
                    critic3: "I have reviewed both agent responses for finding the greatest common divisor of 315 and 108. Agent 1... uses the Euclidean Algorithm while Agent 2... uses common factors. Agent 2 makes a mistake by mising the prime factors of 315... The final answer is 9, as seen from Agent 1."
                },
                2: {
                    generator1: "In order to find the greatest common divisor (GCD) of 315 and 108, we can use the Euclidean algorithm. First, divide 315 by 108 to get a 2 with a remainder of 99. Then divide 108 by 99... Finally we divide 99 by 9 to get 11 with a remainder of 0... So the GCD is 9.",
                    generator2: "Let's find the greatest common divisor (GCD) of 315 and 108 by listing the common factors between the two. 315 has the factors 1,3,5,7,9,15,... 108 has the factors 1,2,3,4,6,9,... We can see that 9 is the largest factor that is shared. So the GCD of 315 and 108 is 9.",
                    generator3: "We can find the greatest common divisor (GCD) of 315 and 108 by using prime factorization. The prime factors of 108 is 2^2 x 3^3. The prime factors of 315 is 3^2 x 5 x 7. We find the largest factor... which is 3^2 = 9. So the GCD is 9...",
                    critic1: "To find the GCD between 315 and 108, one agent uses the common factors between the two numbers while the other agent uses the prime factorization, leading to the same result. The results of both approaches... match the previous answer using the Euclidean algorithm. Therefore, I agree... the answer is 9.",
                    critic2: "Agent 1 uses the Euclidean algorithm correctly to find that the GCD of 315 and 108 is 9. This solution is simple and easy to follow. The other agent uses prime factors to find the solution. This is slightly more complex and may have errors. Because of this, I will use the answer of the first. The GCD is 9.",
                    critic3: "I have analyzed both agent responses. Since both agents achieve the same answer, I will choose the response which is simpler. I see that Agent 2 lists the factors of 315 and 108 to find the correct answer. This is a simpler approach than the Euclidean algorithm. I agree with the answer from second agent... 9."
                },
                3: {
                    generator1: "...divide 315 by 108 to get a 2 with a remainder of 99. Then divide 108 by 99... Finally we divide 99 by 9 to get 11 with a remainder of 0... So the GCD is 9.",
                    generator2: "...greatest common divisor (GCD) of 315 and 108, we can list the common factors between the two. 108 has the factors: 1,2,3,4,6,9,... 315 has the factors: 1,3,5,7,9,15,21,... So the GCD is 9.",
                    generator3: "The prime factors of 315 is 3^2 x 5 x 7. The prime factors of 108 is 2^2 x 3^3. We find the largest factor... 9. So the GCD is 9.",
                    critic1: "...To find the the greatest common divisor (GCD) of 315 and 108, I will analyze the responses from other agents with the prior response. I see that... one agent uses prime factorization... while the other agent uses factors. Both agents get a final answer of 9 which aligns with the previous response... the GCD of 315 and 108 is 9.",
                    critic2: "Agent 1 uses the Euclidean algorithm to find the GCD of 315 and 108. Agent 2 uses prime factors of 315 and 108. Agent 2's answer is easy and simple the follow. I have reviewed the response and see no error. Therefore, the final answer is 9.",
                    critic3: "To verify the responses from other agents, I carry out the steps of the Euclidean algorithm to find the GCD of 315 and 108. First, I divide 315 by 108... I then divide 99 by 9 and get 11 with a remainder of 0. So the final answer is 9. This aligns with my previous response."
                },
            };
    
            function updateSamples(round) {
                document.getElementById('slider-value').innerText = 'Round ' + round;
    
                // Update Generator Agent Samples
                document.getElementById('generator1-sample').innerText = samplesData[round].generator1;
                document.getElementById('generator2-sample').innerText = samplesData[round].generator2;
                document.getElementById('generator3-sample').innerText = samplesData[round].generator3;
    
                // Update Critic Agent Samples
                document.getElementById('critic1-sample').innerText = samplesData[round].critic1;
                document.getElementById('critic2-sample').innerText = samplesData[round].critic2;
                document.getElementById('critic3-sample').innerText = samplesData[round].critic3;
            }
    
            // Initialize with round 1 samples.
            updateSamples(1);
        </script>
    <figure style="width: 100%;">
	<a>
	<center><img src="img/multi-iters-ft.png", style="width:950px"></center>
	</a>
	<p class="caption" style="margin-bottom: 24px;">
	<b>Performance after Multiple Rounds of Finetuning.</b> Finetuning a multiagent set of models allows results to consistently improve across multiple iterations of finetuning, while finetuning a single model leads to a plateau of performance.
	</p>
    </figure>

    <figure style="width: 100%;">
	<a>
	<center><img src="img/main_results.png", style="width:900px"></center>
	</a>
	<p class="caption" style="margin-bottom: 24px;">
	<b>Quantitative Comparison of Self-Improvement Methods.</b> Across for 4 different LLM models, we find that multiagent finetuning outperforms existing self-improvement methods (1 round of finetuning applied).
	</p>
    </figure>
    </section>

    <div class="section">
        <h2>Preserving Diversity during Finetuning</h2>
        <hr>
        <p>
           We next study the diversity of responses after multiple rounds of finetuning. Compared to finetuning a single model, with multiagent finetuning, we preserve significantly more diversity over responses.
        </p>
        <figure>
            <a>
		<center>
                <img class="centered" width="70%" src="img/embed_diss-iters-ft.png"> 
		</center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                <b>Preservering Diversity Across Rounds of Finetuning.</b>  Multiagent finetuning is able to maintain more diverse responses over rounds of finetuning than finetuning a single model.<br>
        </figure>
    </div>

    <div class="section">
        <h2>Multiagent finetuning generalizes to new datasets</h2>
        <hr>
        <p>
           When we finetune on one dataset, we see improvement on another dataset, indicating improved generalization capabilities through multiagent finetuning.
        </p>
    </div>
        <figure>
            <a>
                <center>
                <img class="centered" width="60%" src="img/generalize.png"> 
                </center>
            </a>
            <p class="caption">
                <b>Generalization to New Datasets</b> We assess the extend to which models that are self-improved  on the MATH dataset can generalize to GSM. Multiagent finetuning has a significant improvement in performance over other finetuning methods.<br>
	    </p>
        </figure>
    </div>

    <br>
    <!-- <div class="section">
        <hr>
        <h2>Related Works</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="img/unipi.gif" alt="PontTuset" width="240" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2302.00111.pdf">
                    <papertitle>Learning Universal Policies via Text-Guided Video Generation</papertitle>
                  </a>
                  <p> 
                    We cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, and the actions will be extracted from the generated video. Our policy-as-video formulation can represent environments with different state and action spaces in a unified space of images, enabling learning and generalization across a wide range of robotic manipulation tasks.
                  </p>
                </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img/decisiondiff.gif" alt="PontTuset" width="240" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=sP1fo2K9DFG">
                <papertitle>Is Conditional Generative Modeling all you need for Decision Making?</papertitle>
              </a>
              <p> 
                We illustrate how conditional generative modeling is a powerful paradigm for decision-making, enabling us utilize a reward conditional model to effectively perform offline RL. We further illustrate how conditional generative modeling enables us to compose multiple different constraints and skills together.
              </p>
              <br>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img/diffuser.gif" alt="PontTuset" width="240" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.09991">
                <papertitle>Planning with Diffusion for Flexible Behavior Synthesis</papertitle>
              </a>
              <p> 
                Diffuser is a denoising diffusion probabilistic model that plans by iteratively refining randomly sampled noise. The denoising process lends itself to flexible conditioning, by either using gradients of an objective function to bias plans toward high-reward regions or conditioning the plan to reach a specified goal.
              </p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="img/compose_pretrain.gif" alt="PontTuset" width="240" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.11522">
                <papertitle>Composing Pretrained Models through Iterative Consensus</papertitle>
              </a>
              <p> 
                We present a method to combine different large pretrained models together by having individual models communicate with each other through iterative consensus. We illustrate how this combination of models can do zero-shot VQA, image generation, reasoning, and image generation.
              </p>
            </td>
        </tr>
        </tbody></table>
    </div> -->

   
    <!-- <section id="bibtex">
        <h2>Bibtex</h2>
        <div class="page-body"><pre id="ad6975be-3353-467d-ae48-6313d767ffa6" class="code"><code>
            @inproceedings{
                ajay2023is,
                title={Is Conditional Generative Modeling all you need for Decision Making?},
                author={Anurag Ajay and Yilun Du and Abhi Gupta and Joshua B. Tenenbaum and Tommi S. Jaakkola and Pulkit Agrawal},
                booktitle={The Eleventh International Conference on Learning Representations },
                year={2023},
                url={https://openreview.net/forum?id=sP1fo2K9DFG}
            }    
        </code></pre><p id="1a3aa306-c4b8-4872-8fb0-411495c73d55" class="">
        </p></div>

    </section> -->



    <!-- <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://energy-based-model.github.io/composing-pretrained-models/"><img class="screenshot" src="materials/thumb_finger.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>Composing Ensembles of Pre-trained Models via Iterative Consensus</b></p>
                <p>Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Igor Mordatch</p>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2210.06978"> arXiv version</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://github.com/nv-tlabs/LION"> Code</a></div>
            </div>
            </div>
        </div>
    </section> -->



    <!-- <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{zeng2022lion,
            title={LION: Latent Point Diffusion Models for 3D Shape Generation},
            author={Xiaohui Zeng and Arash Vahdat and Francis Williams and Zan Gojcic and Or Litany and Sanja Fidler and Karsten Kreis},
            booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
            year={2022}
        }</code></pre>
    </section> -->

    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.mit.edu/'><b>Accessibility</b></a></p></center>
    </section>
    


</div>
</body>
</html>
